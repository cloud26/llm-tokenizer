# LLM Tokenizer FastAPI Service

è¿™æ˜¯ä¸€ä¸ªåŸºäº FastAPI çš„ Hugging Face Tokenizer æœåŠ¡ï¼Œæä¾›äº†é«˜æ€§èƒ½çš„æ–‡æœ¬ tokenization åŠŸèƒ½ï¼Œæ”¯æŒç¼“å­˜å’Œå¼‚æ­¥å¤„ç†ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½å¼‚æ­¥å¤„ç†**: ä½¿ç”¨ FastAPI å’Œå¼‚æ­¥ç¼–ç¨‹ï¼Œæ”¯æŒé«˜å¹¶å‘è¯·æ±‚
- ğŸ’¾ **æ™ºèƒ½ç¼“å­˜æœºåˆ¶**: è‡ªåŠ¨ç¼“å­˜å·²åŠ è½½çš„ tokenizerï¼Œé¿å…é‡å¤åŠ è½½
- ğŸ”„ **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒ Hugging Face Hub ä¸Šçš„æ‰€æœ‰ tokenizer æ¨¡å‹
- ğŸ“Š **è¯¦ç»†çš„å“åº”ä¿¡æ¯**: è¿”å› token æ•°é‡ã€tokens åˆ—è¡¨å’Œ token IDs
- ğŸ› ï¸ **å®Œæ•´çš„é”™è¯¯å¤„ç†**: å‹å¥½çš„é”™è¯¯ä¿¡æ¯å’ŒçŠ¶æ€ç 
- ğŸ“ˆ **ç›‘æ§ç«¯ç‚¹**: æä¾›å¥åº·æ£€æŸ¥å’Œç¼“å­˜çŠ¶æ€æŸ¥è¯¢

## å®‰è£…ä¾èµ–

é¦–å…ˆç¡®ä¿æ‚¨å·²ç»å®‰è£…äº† Python 3.8+ï¼Œç„¶åå®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

## å¯åŠ¨æœåŠ¡

### æ–¹æ³• 1: ç›´æ¥è¿è¡Œ

```bash
python main.py
```

### æ–¹æ³• 2: ä½¿ç”¨ uvicorn

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

æœåŠ¡å°†åœ¨ `http://localhost:8000` ä¸Šå¯åŠ¨ã€‚

## API æ–‡æ¡£

å¯åŠ¨æœåŠ¡åï¼Œæ‚¨å¯ä»¥è®¿é—®ä»¥ä¸‹åœ°å€æŸ¥çœ‹è‡ªåŠ¨ç”Ÿæˆçš„ API æ–‡æ¡£ï¼š

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## API ç«¯ç‚¹

### 1. æ–‡æœ¬ Tokenization

**POST** `/tokenize`

å°†æ–‡æœ¬è½¬æ¢ä¸º tokensã€‚

**è¯·æ±‚ä½“:**
```json
{
    "text": "Hello, how are you today?",
    "hubPath": "bert-base-uncased"
}
```

**å“åº”:**
```json
{
    "success": true,
    "tokenCount": 8,
    "tokens": ["hello", ",", "how", "are", "you", "today", "?"],
    "tokenIds": [7592, 1010, 2129, 2024, 2017, 2651, 1029]
}
```

### 2. å¥åº·æ£€æŸ¥

**GET** `/health`

æ£€æŸ¥æœåŠ¡çŠ¶æ€ã€‚

**å“åº”:**
```json
{
    "status": "healthy",
    "cache_size": 3
}
```

### 3. ç¼“å­˜ä¿¡æ¯

**GET** `/cache`

æŸ¥çœ‹å½“å‰ç¼“å­˜çš„æ¨¡å‹ä¿¡æ¯ã€‚

**å“åº”:**
```json
{
    "cached_models": ["bert-base-uncased", "gpt2", "distilbert-base-uncased"],
    "cache_size": 3
}
```

### 4. æ¸…é™¤ç¼“å­˜

**DELETE** `/cache`

æ¸…é™¤æ‰€æœ‰ç¼“å­˜çš„ tokenizerã€‚

**å“åº”:**
```json
{
    "message": "Cleared 3 tokenizers from cache"
}
```

## æ”¯æŒçš„æ¨¡å‹

è¯¥æœåŠ¡æ”¯æŒ Hugging Face Hub ä¸Šçš„æ‰€æœ‰ tokenizer æ¨¡å‹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š

- `bert-base-uncased`
- `bert-large-uncased`
- `gpt2`
- `distilbert-base-uncased`
- `roberta-base`
- `t5-small`
- `microsoft/DialoGPT-medium`
- ç­‰ç­‰...

## ä½¿ç”¨ç¤ºä¾‹

### Python å®¢æˆ·ç«¯ç¤ºä¾‹

```python
import requests

# API åŸºç¡€ URL
BASE_URL = "http://localhost:8000"

# å‘é€ tokenization è¯·æ±‚
response = requests.post(
    f"{BASE_URL}/tokenize",
    json={
        "text": "Hello, world!",
        "hubPath": "bert-base-uncased"
    }
)

if response.status_code == 200:
    result = response.json()
    print(f"Token count: {result['tokenCount']}")
    print(f"Tokens: {result['tokens']}")
else:
    print(f"Error: {response.text}")
```

### cURL ç¤ºä¾‹

```bash
curl -X POST "http://localhost:8000/tokenize" \
     -H "Content-Type: application/json" \
     -d '{
       "text": "Hello, world!",
       "hubPath": "bert-base-uncased"
     }'
```

### JavaScript/Fetch ç¤ºä¾‹

```javascript
const response = await fetch('http://localhost:8000/tokenize', {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json',
    },
    body: JSON.stringify({
        text: 'Hello, world!',
        hubPath: 'bert-base-uncased'
    })
});

const result = await response.json();
console.log('Token count:', result.tokenCount);
console.log('Tokens:', result.tokens);
```

## æµ‹è¯•

è¿è¡Œæµ‹è¯•è„šæœ¬æ¥éªŒè¯æœåŠ¡åŠŸèƒ½ï¼š

```bash
# ç¡®ä¿æœåŠ¡å·²å¯åŠ¨ï¼Œç„¶åè¿è¡Œæµ‹è¯•
python test_api.py
```

æµ‹è¯•è„šæœ¬å°†éªŒè¯ï¼š
- å¥åº·æ£€æŸ¥ç«¯ç‚¹
- åŸºæœ¬ tokenization åŠŸèƒ½
- ç¼“å­˜æœºåˆ¶
- ä¸åŒæ¨¡å‹æ”¯æŒ
- é”™è¯¯å¤„ç†

## æ€§èƒ½ä¼˜åŒ–

1. **ç¼“å­˜æœºåˆ¶**: é¦–æ¬¡åŠ è½½çš„ tokenizer ä¼šè¢«ç¼“å­˜ï¼Œåç»­è¯·æ±‚ç›´æ¥ä½¿ç”¨ç¼“å­˜
2. **å¼‚æ­¥å¤„ç†**: ä½¿ç”¨å¼‚æ­¥ I/O æé«˜å¹¶å‘æ€§èƒ½
3. **çº¿ç¨‹æ± **: CPU å¯†é›†å‹çš„ tokenization æ“ä½œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ

## é”™è¯¯å¤„ç†

æœåŠ¡æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼š

- `400 Bad Request`: ç¼ºå°‘å¿…éœ€å‚æ•°
- `500 Internal Server Error`: æ¨¡å‹åŠ è½½å¤±è´¥æˆ– tokenization é”™è¯¯

## æ—¥å¿—

æœåŠ¡ä¼šè®°å½•è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- æ¨¡å‹åŠ è½½çŠ¶æ€
- ç¼“å­˜ä½¿ç”¨æƒ…å†µ
- é”™è¯¯ä¿¡æ¯

## éƒ¨ç½²å»ºè®®

### Docker éƒ¨ç½²

å¯ä»¥åˆ›å»º Dockerfile è¿›è¡Œå®¹å™¨åŒ–éƒ¨ç½²ï¼š

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### ç”Ÿäº§ç¯å¢ƒ

ç”Ÿäº§ç¯å¢ƒå»ºè®®ï¼š
- ä½¿ç”¨ Gunicorn + Uvicorn workers
- é…ç½®åå‘ä»£ç† (Nginx)
- å¯ç”¨æ—¥å¿—è½®è½¬
- è®¾ç½®é€‚å½“çš„ç¼“å­˜ç­–ç•¥

## è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº MIT è®¸å¯è¯å¼€æºã€‚ # llm-tokenizer
